{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load SpaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize DeepInfra OpenAI Client\n",
    "openai = OpenAI(\n",
    "    api_key=\"YOUR_API_KEY\",  # Replace with your key\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_size=200, min_chunk_size=100):\n",
    "    # Step 1: Use SpaCy for sentence splitting\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # Step 2: Group sentences into chunks\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # If adding the sentence exceeds max size, save the current chunk\n",
    "        if len(current_chunk) + len(sentence) > max_chunk_size:\n",
    "            # If current_chunk is too small, add more until it meets min size\n",
    "            if len(current_chunk) < min_chunk_size:\n",
    "                current_chunk += \" \" + sentence\n",
    "            else:\n",
    "                # Save the chunk and start a new one\n",
    "                chunks.append(current_chunk.strip())\n",
    "                current_chunk = sentence\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence\n",
    "\n",
    "    # Add the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    # Step 3: Handle very large sentences by word splitting\n",
    "    final_chunks = []\n",
    "    for chunk in chunks:\n",
    "        if len(chunk) > max_chunk_size:\n",
    "            words = chunk.split()  # Split by words\n",
    "            temp_chunk = \"\"\n",
    "            for word in words:\n",
    "                if len(temp_chunk) + len(word) + 1 > max_chunk_size:\n",
    "                    final_chunks.append(temp_chunk.strip())\n",
    "                    temp_chunk = word\n",
    "                else:\n",
    "                    temp_chunk += \" \" + word\n",
    "            if temp_chunk:\n",
    "                final_chunks.append(temp_chunk.strip())\n",
    "        else:\n",
    "            final_chunks.append(chunk)\n",
    "\n",
    "    return final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_faiss_index(index, index_path, metadata_path, metadata):\n",
    "    # Save index and metadata\n",
    "    faiss.write_index(index, index_path)\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(\"FAISS index and metadata saved locally.\")\n",
    "\n",
    "\n",
    "def load_faiss_index(index_path, metadata_path):\n",
    "    # Load index and metadata\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return index, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks):\n",
    "    # Generate embeddings for each chunk with BgeM3 model\n",
    "    embeddings = []\n",
    "    for chunk in chunks:\n",
    "        response = openai.embeddings.create(\n",
    "            model=\"BAAI/bge-m3\",\n",
    "            input=chunk,\n",
    "            encoding_format=\"float\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_store_faiss(chunks, index_path='faiss_index.bin', metadata_path='metadata.pkl'):\n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    index.add(embeddings)  # Add embeddings to FAISS index\n",
    "\n",
    "    # Save metadata and index\n",
    "    save_faiss_index(index, index_path, metadata_path, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss(query, top_k=3, index_path='faiss_index.bin', metadata_path='metadata.pkl'):\n",
    "    # Load FAISS index and metadata\n",
    "    index, metadata = load_faiss_index(index_path, metadata_path)\n",
    "\n",
    "    # Generate embedding for query\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"BAAI/bge-m3\",\n",
    "        input=query,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    query_embedding = np.array([response.data[0].embedding])\n",
    "\n",
    "    # Search FAISS index\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nSearch Results:\")\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        print(f\"Rank {i+1}: {metadata[idx]} (Distance: {distances[0][i]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunks Created: 2002\n",
      "FAISS index and metadata saved locally.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Load text file\n",
    "    with open('knowledge_base/istanbul_places_content.txt', 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Set chunk size\n",
    "    max_chunk_size = 500\n",
    "    min_chunk_size = 100\n",
    "\n",
    "    # Perform recursive chunking\n",
    "    chunks = chunk_text(text, max_chunk_size, min_chunk_size)\n",
    "    print(f\"Total Chunks Created: {len(chunks)}\")\n",
    "\n",
    "    # Process chunks and store in FAISS\n",
    "    process_and_store_faiss(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search Results:\n",
      "Rank 1: Ticket Price: 3€ (Euro) State: Closed Official Announcements: Fethiye (Pammakaristos Church) Mosque/Museum: The Monastery of the Theotokos Pammakaristos (Mother of God the All-Blessed), now Fethiye Mosque, was located on the fifth hill of Constantinople, in the modern neighborhood of Çarsamba. It is located southeast of Chora and Blachernai churches. (Distance: 0.7436721324920654)\n",
      "Rank 2: A document of the second half of the 16th century describes a number of tombs and relics there, including Alexios Komnenos. In 1587, it was converted into a mosque, after which it was significantly altered. It was converted into a mosque around 1587 during the reign of Murad III. It was called Fethiye (“Conquest”) Mosque, in commemoration of the Ottoman conquest of Georgia. Sinan Pasha, then the Grand Vizier, established its madrasa. (Distance: 0.7661095857620239)\n",
      "Rank 3: 124-125\n",
      "Points from Turkey\n",
      "The Fatih Mosque (Turkish: Fatih Camii, \"Conqueror's Mosque\" in English) is an Ottoman mosque off Fevzi Paşa Caddesi in the Fatih district of Istanbul, Turkey. The original mosque was constructed between 1463 and 1470 on the site of the Church of the Holy Apostles. Seriously damaged in the 1766 earthquake, it was rebuilt in 1771 to a different design. (Distance: 0.8072611093521118)\n"
     ]
    }
   ],
   "source": [
    " # Perform a test search\n",
    "query = \"Fethiye Mosque\"\n",
    "search_faiss(query, top_k=3, index_path='faiss_index_recursive.bin', metadata_path='metadata_recursive.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
