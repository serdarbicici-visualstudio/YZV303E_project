{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:15.601168Z",
     "iopub.status.busy": "2025-01-05T18:10:15.600842Z",
     "iopub.status.idle": "2025-01-05T18:10:22.578738Z",
     "shell.execute_reply": "2025-01-05T18:10:22.577778Z",
     "shell.execute_reply.started": "2025-01-05T18:10:15.601137Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch.nn.functional as F\n",
    "from eval_metrics import evaluate_single_input\n",
    "import os \n",
    "os.environ['HF_TOKEN'] = \"YOUR_HF_TOKEN\"\n",
    "openai = OpenAI(\n",
    "    api_key=\"YOUR_API_KEY\",  # Replace with your actual API key\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "# Define the model name\n",
    "model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:22.663786Z",
     "iopub.status.busy": "2025-01-05T18:10:22.663591Z",
     "iopub.status.idle": "2025-01-05T18:10:22.667719Z",
     "shell.execute_reply": "2025-01-05T18:10:22.666819Z",
     "shell.execute_reply.started": "2025-01-05T18:10:22.663767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_faiss_index(index_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Loads the FAISS index and metadata from local storage.\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(index_path)\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return index, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:22.668835Z",
     "iopub.status.busy": "2025-01-05T18:10:22.668557Z",
     "iopub.status.idle": "2025-01-05T18:10:22.680688Z",
     "shell.execute_reply": "2025-01-05T18:10:22.679858Z",
     "shell.execute_reply.started": "2025-01-05T18:10:22.668807Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def search_faiss(query, top_k, index_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Searches FAISS for the most relevant chunks to the query.\n",
    "    \"\"\"\n",
    "    # Load index and metadata\n",
    "    index, metadata = load_faiss_index(index_path, metadata_path)\n",
    "\n",
    "    # Generate embedding for the query\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"BAAI/bge-m3\",\n",
    "        input=query,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    query_embedding = np.array([response.data[0].embedding])\n",
    "\n",
    "    # Search FAISS index for the top_k results\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Retrieve matching chunks\n",
    "    results = []\n",
    "    for i, idx in enumerate(indices[0]):\n",
    "        results.append((metadata[idx], distances[0][i]))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:22.681793Z",
     "iopub.status.busy": "2025-01-05T18:10:22.681503Z",
     "iopub.status.idle": "2025-01-05T18:10:29.817658Z",
     "shell.execute_reply": "2025-01-05T18:10:29.816984Z",
     "shell.execute_reply.started": "2025-01-05T18:10:22.681764Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "dataset = load_dataset(\n",
    "path='BlackFear/istanbul-qa-dataset',\n",
    "trust_remote_code=True,\n",
    ")\n",
    "\n",
    "data = dataset['test']\n",
    "\n",
    "# Extract questions and answers\n",
    "queries = data['question']\n",
    "references = data['reference']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.818695Z",
     "iopub.status.busy": "2025-01-05T18:10:29.818450Z",
     "iopub.status.idle": "2025-01-05T18:10:29.822997Z",
     "shell.execute_reply": "2025-01-05T18:10:29.822215Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.818673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_answer(query, context):\n",
    "    \"\"\"\n",
    "    Generates an answer based on the query and retrieved context using the LLM.\n",
    "    \"\"\"\n",
    "    # Prepare the input prompt\n",
    "    prompt = f\"\"\"\n",
    "    Use the following contexts to answer the question:\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Question:\n",
    "    {query}\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "\n",
    "    # Query the Llama model\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You will be answering questions about Istanbul. Please provide the answer to the following question.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.825510Z",
     "iopub.status.busy": "2025-01-05T18:10:29.825284Z",
     "iopub.status.idle": "2025-01-05T18:10:29.836966Z",
     "shell.execute_reply": "2025-01-05T18:10:29.836194Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.825490Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_bge():\n",
    "    \"\"\"\n",
    "    Loads BGE reranker from HuggingFace.\n",
    "    \"\"\"\n",
    "    model_name = \"BAAI/bge-small-en-v1.5\"  \n",
    "    model = SentenceTransformer(model_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.838363Z",
     "iopub.status.busy": "2025-01-05T18:10:29.838172Z",
     "iopub.status.idle": "2025-01-05T18:10:29.850766Z",
     "shell.execute_reply": "2025-01-05T18:10:29.850019Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.838346Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_tildev2():\n",
    "    \"\"\"\n",
    "    Loads TildeV2 reranker from HuggingFace.\n",
    "    \"\"\"\n",
    "    model_name = \"ielab/TILDEv2-TILDE128-exp\"  \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.851632Z",
     "iopub.status.busy": "2025-01-05T18:10:29.851388Z",
     "iopub.status.idle": "2025-01-05T18:10:29.867533Z",
     "shell.execute_reply": "2025-01-05T18:10:29.866735Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.851611Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def hyde(query):\n",
    "    \"\"\"\n",
    "    Generates a hypothetical document based on the query using LLM (Query2Doc).\n",
    "    Combines the query and hypothetical document for retrieval.\n",
    "    \"\"\"\n",
    "    # Prompt LLM to create a hypothetical document\n",
    "    prompt = f\"\"\"\n",
    "    Generate a hypothetical paragraph based on the following query.\n",
    "    Just give the created paragraph, no need to answer the question.\n",
    "    Query: {query}\n",
    "\n",
    "    Hypothetical Paragraph:\n",
    "    \"\"\"\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': prompt}],\n",
    "    )\n",
    "    hypothetical_doc = response.choices[0].message.content.strip()\n",
    "\n",
    "    # Combine query with the hypothetical document\n",
    "    return hypothetical_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.868632Z",
     "iopub.status.busy": "2025-01-05T18:10:29.868365Z",
     "iopub.status.idle": "2025-01-05T18:10:29.882639Z",
     "shell.execute_reply": "2025-01-05T18:10:29.881791Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.868612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def hybrid_search(query, top_k, index_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Combines dense (FAISS) and sparse (BM25) retrieval for better results.\n",
    "    \"\"\"\n",
    "    # Load FAISS index and metadata\n",
    "    index, metadata = load_faiss_index(index_path, metadata_path)\n",
    "\n",
    "    # Dense Retrieval (FAISS)\n",
    "    # Generate embedding for query\n",
    "    response = openai.embeddings.create(\n",
    "        model=\"BAAI/bge-m3\",  # Dense embedding model\n",
    "        input=query,\n",
    "        encoding_format=\"float\"\n",
    "    )\n",
    "    query_embedding = np.array([response.data[0].embedding])\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "\n",
    "    # Sparse Retrieval (BM25)\n",
    "    tokenized_metadata = [doc.split() for doc in metadata]  # Preprocess metadata\n",
    "    bm25 = BM25Okapi(tokenized_metadata)\n",
    "    sparse_scores = bm25.get_scores(query.split())\n",
    "\n",
    "    # Normalize and combine scores (Hybrid Search)\n",
    "    dense_scores = 1 / (1 + distances[0])  # Convert FAISS distances to similarity\n",
    "    sparse_scores = np.array(sparse_scores)\n",
    "    sparse_scores = sparse_scores / np.max(sparse_scores)  # Normalize BM25 scores\n",
    "\n",
    "    # Combine dense and sparse scores (50-50 weight)\n",
    "    combined_scores = 0.5 * dense_scores + 0.5 * sparse_scores[indices[0]]\n",
    "\n",
    "    # Sort results based on combined scores\n",
    "    sorted_indices = np.argsort(-combined_scores)  # Descending order\n",
    "    results = [(metadata[indices[0][i]], combined_scores[i]) for i in sorted_indices]\n",
    "\n",
    "    return results[:top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.883498Z",
     "iopub.status.busy": "2025-01-05T18:10:29.883265Z",
     "iopub.status.idle": "2025-01-05T18:10:29.893092Z",
     "shell.execute_reply": "2025-01-05T18:10:29.892360Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.883466Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def bge_rerank(query, results, model):\n",
    "    \"\"\"\n",
    "    Reranks results using BGE embeddings.\n",
    "    \"\"\"\n",
    "    # Encode query and results\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    doc_embeddings = model.encode([res[0] for res in results], convert_to_tensor=True)\n",
    "\n",
    "    # Compute similarity\n",
    "    scores = util.pytorch_cos_sim(query_embedding, doc_embeddings)[0]\n",
    "    reranked = sorted(zip([res[0] for res in results], scores.tolist()), key=lambda x: x[1], reverse=True)\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.894096Z",
     "iopub.status.busy": "2025-01-05T18:10:29.893840Z",
     "iopub.status.idle": "2025-01-05T18:10:29.907634Z",
     "shell.execute_reply": "2025-01-05T18:10:29.906895Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.894063Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tildev2_rerank(query, results, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Reranks results using TildeV2 embeddings and cosine similarity.\n",
    "    \"\"\"\n",
    "    reranked = []\n",
    "\n",
    "    # Process the query\n",
    "    query_inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    query_outputs = model(**query_inputs)\n",
    "    query_embedding = query_outputs.last_hidden_state.mean(dim=1)  # Take mean of embeddings\n",
    "\n",
    "    for doc in results:\n",
    "        # Process each document\n",
    "        doc_inputs = tokenizer(doc[0], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        doc_outputs = model(**doc_inputs)\n",
    "        doc_embedding = doc_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        # Calculate cosine similarity\n",
    "        score = F.cosine_similarity(query_embedding, doc_embedding).item()\n",
    "        reranked.append((doc[0], score))\n",
    "\n",
    "    # Sort by scores in descending order\n",
    "    reranked = sorted(reranked, key=lambda x: x[1], reverse=True)\n",
    "    return reranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.908669Z",
     "iopub.status.busy": "2025-01-05T18:10:29.908439Z",
     "iopub.status.idle": "2025-01-05T18:10:29.923632Z",
     "shell.execute_reply": "2025-01-05T18:10:29.922814Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.908638Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def repack_results(results, method='forward'):\n",
    "    \"\"\"\n",
    "    Repack results based on specified method: 'forward', 'reverse', or 'side'.\n",
    "    \"\"\"\n",
    "    if method == 'forward':\n",
    "        # Sequentially combine results\n",
    "        context = \"\\n\\n\".join([f\"Chunk {i+1}: {result[0]}\" for i, result in enumerate(results)])\n",
    "\n",
    "    elif method == 'reverse':\n",
    "        # Start from the most relevant and work backward\n",
    "        context = \"\\n\\n\".join([f\"Chunk {i+1}: {result[0]}\" for i, result in enumerate(results[::-1])])\n",
    "\n",
    "    elif method == 'side':\n",
    "        # Alternate between high and low relevance results\n",
    "        mid = len(results) // 2\n",
    "        merged = []\n",
    "        for i in range(mid):\n",
    "            merged.append(results[i])  # High rank\n",
    "            if i + mid < len(results):\n",
    "                merged.append(results[i + mid])  # Low rank\n",
    "\n",
    "        context = \"\\n\\n\".join([f\"Chunk {i+1}: {res[0]}\" for i, res in enumerate(merged)])\n",
    "    else:\n",
    "        raise ValueError(\"Invalid repack method. Choose 'forward', 'reverse', or 'side'.\")\n",
    "\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.924543Z",
     "iopub.status.busy": "2025-01-05T18:10:29.924293Z",
     "iopub.status.idle": "2025-01-05T18:10:29.937258Z",
     "shell.execute_reply": "2025-01-05T18:10:29.936535Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.924513Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def summarize_context(query, context):\n",
    "    \"\"\"\n",
    "    Summarizes the retrieved context into a concise and representative format using GPT.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Summarize the following context to make it concise yet representative of the main ideas, keeping it relevant to the query.\n",
    "    Just give the summarized content.\n",
    "\n",
    "    Query: {query}\n",
    "\n",
    "    Context:\n",
    "    {context}\n",
    "\n",
    "    Summary:\n",
    "    \"\"\"\n",
    "    # Use GPT to generate the summary\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    # Extract and return the summary\n",
    "    summary = response.choices[0].message.content.strip()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.938095Z",
     "iopub.status.busy": "2025-01-05T18:10:29.937911Z",
     "iopub.status.idle": "2025-01-05T18:10:29.950107Z",
     "shell.execute_reply": "2025-01-05T18:10:29.949439Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.938078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def rag_pipeline(query, \n",
    "                 top_k=5, \n",
    "                 method=\"hyde\", \n",
    "                 retrive_method='hybrid', \n",
    "                 rerank_method='monot5', \n",
    "                 repack_method='forward',\n",
    "                 summarize=True,\n",
    "                 index_path='index.faiss', \n",
    "                 metadata_path='metadata.pkl'):\n",
    "    \"\"\"\n",
    "    Executes the RAG pipeline with retrieval, reranking, repacking, and summarization options.\n",
    "    \"\"\"\n",
    "    if rerank_method == 'bge':\n",
    "        model = load_bge()\n",
    "    elif rerank_method == 'tildev2':\n",
    "        tokenizer, model = load_tildev2()\n",
    "\n",
    "    # Enhance query using HyDE or Query2Doc\n",
    "    if method == 'query2doc':\n",
    "        query = query + \" \" + hyde(query)\n",
    "    elif method == 'hyde':\n",
    "        query = hyde(query)\n",
    "\n",
    "    # Retrieve initial results\n",
    "    if retrive_method == 'hybrid':\n",
    "        results = hybrid_search(query, top_k, index_path, metadata_path)\n",
    "    else:\n",
    "        results = search_faiss(query, top_k, index_path, metadata_path)\n",
    "\n",
    "    # Apply reranking\n",
    "    if rerank_method == 'bge':\n",
    "        results = bge_rerank(query, results, model)\n",
    "    elif rerank_method == 'tildev2':\n",
    "        results = tildev2_rerank(query, results, tokenizer, model)\n",
    "\n",
    "    # **Apply Repacking** after reranking\n",
    "    context = repack_results(results, repack_method)\n",
    "\n",
    "    # **Summarization Step**\n",
    "    if summarize:\n",
    "        context = summarize_context(query, context)  # Summarize the context\n",
    "\n",
    "    # Generate an answer using LLM\n",
    "    answer = generate_answer(query, context)\n",
    "\n",
    "    return answer, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-01-05T18:10:29.951050Z",
     "iopub.status.busy": "2025-01-05T18:10:29.950799Z",
     "iopub.status.idle": "2025-01-05T18:10:52.680921Z",
     "shell.execute_reply": "2025-01-05T18:10:52.679327Z",
     "shell.execute_reply.started": "2025-01-05T18:10:29.951030Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "scores = []\n",
    "for i in tqdm(range(len(queries)), desc=\"Processing queries\", total=len(queries)):\n",
    "    query = queries[i]\n",
    "    reference = references[i]\n",
    "    answer,context = rag_pipeline(query, top_k=8, method='',\n",
    "                                  retrive_method='',rerank_method='tildev2',repack_method='forward',\n",
    "                                  summarize=True,index_path='/kaggle/input/database-recursive/faiss_index_semantic.index',\n",
    "                                  metadata_path='/kaggle/input/database-recursive/metadata_semantic.pkl')\n",
    "    score = evaluate_single_input(query, reference, answer, context)\n",
    "    scores.append(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-05T18:10:52.681550Z",
     "iopub.status.idle": "2025-01-05T18:10:52.681848Z",
     "shell.execute_reply": "2025-01-05T18:10:52.681730Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Calculate average scores\n",
    "faithfullness, relevance, factual, overall = 0, 0, 0, 0\n",
    "for i, score in enumerate(scores):\n",
    "    faithfullness += scores[i]['Faithfulness']\n",
    "    relevance += scores[i]['Relevancy']\n",
    "    factual += scores[i]['Factual Correctness']\n",
    "    overall += scores[i]['Overall Score']\n",
    "\n",
    "faithfullness /= len(scores)\n",
    "relevance /= len(scores)\n",
    "factual /= len(scores)\n",
    "overall /= len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-05T18:10:52.683120Z",
     "iopub.status.idle": "2025-01-05T18:10:52.683464Z",
     "shell.execute_reply": "2025-01-05T18:10:52.683316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display the average scores\n",
    "print(f\"{faithfullness}, {relevance}, {factual}, {overall}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6425339,
     "sourceId": 10376631,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
