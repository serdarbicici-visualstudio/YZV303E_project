{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "import spacy\n",
    "\n",
    "# Initialize DeepInfra OpenAI Client\n",
    "openai = OpenAI(\n",
    "    api_key=\"2Q2AU9IG4jKLdqmRrHc2UxaLP8hHB0ii\",  # Replace with your key\n",
    "    base_url=\"https://api.deepinfra.com/v1/openai\",\n",
    ")\n",
    "\n",
    "\n",
    "def generate_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Generates embeddings using DeepInfra's BAAI/bge-m3 model with progress bar.\n",
    "    Args:\n",
    "        chunks (list): List of text chunks.\n",
    "    Returns:\n",
    "        np.ndarray: Array of embeddings.\n",
    "    \"\"\"\n",
    "    embeddings = []\n",
    "    for chunk in tqdm(chunks, desc=\"Generating Embeddings\"):\n",
    "        try:\n",
    "            # Generate embeddings\n",
    "            response = openai.embeddings.create(\n",
    "                model=\"BAAI/bge-m3\",\n",
    "                input=chunk,\n",
    "                encoding_format=\"float\"\n",
    "            )\n",
    "            # Append embedding\n",
    "            embeddings.append(response.data[0].embedding)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for chunk: {chunk[:30]}... - {e}\")\n",
    "            embeddings.append([0] * 768)  # Add zero vector for failed chunks\n",
    "    return np.array(embeddings, dtype=np.float32)  # Ensure float32 type\n",
    "\n",
    "\n",
    "def process_and_store_faiss(chunks, index_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Processes the text chunks, generates embeddings, and stores them in FAISS.\n",
    "    \"\"\"\n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(chunks)\n",
    "\n",
    "    # Create FAISS index\n",
    "    dimension = embeddings.shape[1]  # Get embedding dimension\n",
    "    index = faiss.IndexFlatL2(dimension)  # Create FAISS index with L2 distance\n",
    "    index.add(embeddings)  # Add embeddings to the index\n",
    "\n",
    "    # Save index and metadata\n",
    "    save_faiss_index(index, index_path, metadata_path, chunks)\n",
    "\n",
    "\n",
    "def save_faiss_index(index, index_path, metadata_path, metadata):\n",
    "    \"\"\"\n",
    "    Saves the FAISS index and metadata locally.\n",
    "    \"\"\"\n",
    "    # Save FAISS index\n",
    "    faiss.write_index(index, index_path)\n",
    "    # Save metadata\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    print(\"FAISS index and metadata saved locally.\")\n",
    "\n",
    "\n",
    "def load_faiss_index(index_path, metadata_path):\n",
    "    \"\"\"\n",
    "    Loads the FAISS index and metadata from local storage.\n",
    "    \"\"\"\n",
    "    # Load FAISS index\n",
    "    index = faiss.read_index(index_path)\n",
    "    # Load metadata\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    return index, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Embeddings: 100%|██████████| 4997/4997 [40:07<00:00,  2.08it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index and metadata saved locally.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the pre-trained NLP model\n",
    "# install en_core_web_sm model\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Use 'en_core_web_lg' for better accuracy with larger models\n",
    "\n",
    "# Read the text file\n",
    "with open('istanbul_places_content.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "\n",
    "def semantic_chunking(text, max_length=500, overlap=50):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    for sent in doc.sents:  # Iterate through sentences\n",
    "        # Add the sentence if it fits in the current chunk\n",
    "        if len(current_chunk) + len(sent.text) <= max_length:\n",
    "            current_chunk += sent.text + \" \"\n",
    "        else:\n",
    "            # Add the current chunk to the list and start a new chunk with overlap\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \" \".join(current_chunk.split()[-overlap:]) + \" \" + sent.text\n",
    "    \n",
    "    # Append the last chunk\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Perform semantic chunking\n",
    "chunks = semantic_chunking(text)\n",
    "\n",
    "# Define the file paths for saving the FAISS index and metadata\n",
    "index_path = \"faiss_index.index\"\n",
    "metadata_path = \"metadata.pkl\"\n",
    "\n",
    "# Process and store the FAISS index\n",
    "process_and_store_faiss(chunks, index_path, metadata_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda_deneme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
